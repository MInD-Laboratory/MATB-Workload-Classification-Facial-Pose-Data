{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Linear Metrics Statistical Analysis\n",
    "\n",
    "This notebook performs comprehensive statistical analysis on all pose linear metrics using mixed-effects models and generates publication-ready figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/raw_data/participant_info.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m warnings.filterwarnings(\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Load session information\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m Session_Info = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mraw_data\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparticipant_info.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Add session_order column to Session_Info\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mSession1\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSession2\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSession3\u001b[39m\u001b[33m\"\u001b[39m}.issubset(Session_Info.columns):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/matb-analysis/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/matb-analysis/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/matb-analysis/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/matb-analysis/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/matb-analysis/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/raw_data/participant_info.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.ticker import MaxNLocator, FormatStrFormatter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load session information\n",
    "Session_Info = pd.read_csv(Path(\"data\") / \"pose_data\" / \"participant_info.csv\")\n",
    "\n",
    "# Add session_order column to Session_Info\n",
    "if {\"session01\", \"session02\", \"session03\"}.issubset(Session_Info.columns):\n",
    "    Session_Info[\"session_order\"] = (\n",
    "        Session_Info[\"session01\"].str[0] +\n",
    "        Session_Info[\"session02\"].str[0] +\n",
    "        Session_Info[\"session03\"].str[0]\n",
    "    )\n",
    "\n",
    "# Prepare session_order and session_order_numeric maps\n",
    "session_order_numeric_map = {\"LMH\": 1, \"LHM\": 2}\n",
    "session_order_map = {}\n",
    "if \"session_order\" in Session_Info.columns:\n",
    "    session_order_map = Session_Info.set_index(\"Participant ID\")[\"session_order\"].to_dict()\n",
    "\n",
    "# Standardize condition values\n",
    "cond_map = {\"low\": \"L\", \"moderate\": \"M\", \"hard\": \"H\", \"l\": \"L\", \"m\": \"M\", \"h\": \"H\"}\n",
    "\n",
    "print(\"Session information loaded successfully\")\n",
    "print(f\"Total participants: {len(Session_Info)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all three normalization methods for pose linear metrics\n",
    "linear_metrics_dir = Path(\"data\") / \"processed\" / \"linear_metrics\"\n",
    "\n",
    "pose_data = {}\n",
    "\n",
    "# Load each normalization method if it exists\n",
    "for method in [\"original\", \"procrustes_global\", \"procrustes_participant\"]:\n",
    "    file_path = linear_metrics_dir / f\"{method}_linear.csv\"\n",
    "    if file_path.exists():\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Add session order information\n",
    "        if \"participant\" in df.columns and session_order_map:\n",
    "            df[\"session_order\"] = df[\"participant\"].map(session_order_map)\n",
    "            df[\"session_order_numeric\"] = df[\"session_order\"].map(session_order_numeric_map)\n",
    "        \n",
    "        # Standardize condition values\n",
    "        if \"condition\" in df.columns:\n",
    "            df[\"condition\"] = df[\"condition\"].astype(str).str.strip().str.upper()\n",
    "            # Ensure L, M, H format\n",
    "            df[\"condition\"] = df[\"condition\"].map(lambda x: cond_map.get(x.lower(), x))\n",
    "        \n",
    "        pose_data[method] = df\n",
    "        print(f\"Loaded {method}: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    else:\n",
    "        print(f\"Warning: {method}_linear.csv not found\")\n",
    "\n",
    "# Get list of all metric columns (excluding metadata)\n",
    "metadata_cols = {\"source\", \"participant\", \"condition\", \"window_index\", \"t_start_frame\", \"t_end_frame\", \n",
    "                \"session_order\", \"session_order_numeric\"}\n",
    "\n",
    "if pose_data:\n",
    "    # Get metrics from first available dataset\n",
    "    sample_df = next(iter(pose_data.values()))\n",
    "    all_metric_cols = [col for col in sample_df.columns if col not in metadata_cols]\n",
    "    print(f\"\\nFound {len(all_metric_cols)} total metrics\")\n",
    "    print(\"All available metrics:\")\n",
    "    for i, metric in enumerate(all_metric_cols, 1):\n",
    "        print(f\"  {i:2d}. {metric}\")\n",
    "\n",
    "# ===== CONFIGURATION: SELECT METRICS TO ANALYZE =====\n",
    "# You can modify this section to choose which metrics to analyze and plot\n",
    "\n",
    "# Option 1: Analyze ALL metrics (default)\n",
    "ANALYZE_ALL_METRICS = True\n",
    "\n",
    "# Option 2: Analyze specific metrics by name\n",
    "SELECTED_METRICS = [\n",
    "    # Head rotation metrics\n",
    "    \"head_rotation_rad_mean_abs_vel\",\n",
    "    \"head_rotation_rad_rms\", \n",
    "    \n",
    "    # Blink metrics\n",
    "    \"blink_aperture_mean_abs_vel\",\n",
    "    \"blink_aperture_rms\",\n",
    "    \n",
    "    # Mouth metrics  \n",
    "    \"mouth_aperture_mean_abs_vel\",\n",
    "    \"mouth_aperture_rms\",\n",
    "    \n",
    "    # Pupil metrics\n",
    "    \"pupil_dx_rms\",\n",
    "    \"pupil_dy_rms\",\n",
    "    \"pupil_metric_rms\",\n",
    "    \n",
    "    # Center face metrics\n",
    "    \"center_face_magnitude_rms\",\n",
    "    \"center_face_x_rms\", \n",
    "    \"center_face_y_rms\"\n",
    "]\n",
    "\n",
    "# Option 3: Analyze metrics by pattern (e.g., only RMS metrics)\n",
    "METRIC_PATTERNS = [\"_rms\"]  # [\"_rms\", \"_mean_abs_vel\", \"_mean_abs_acc\"]\n",
    "\n",
    "# Choose which metrics to use\n",
    "if ANALYZE_ALL_METRICS:\n",
    "    metric_cols = all_metric_cols\n",
    "    print(f\"\\nWill analyze ALL {len(metric_cols)} metrics\")\n",
    "else:\n",
    "    if METRIC_PATTERNS:\n",
    "        # Filter by patterns\n",
    "        metric_cols = [col for col in all_metric_cols \n",
    "                      if any(pattern in col for pattern in METRIC_PATTERNS)]\n",
    "        print(f\"\\nWill analyze {len(metric_cols)} metrics matching patterns {METRIC_PATTERNS}\")\n",
    "    else:\n",
    "        # Use selected metrics\n",
    "        metric_cols = [col for col in SELECTED_METRICS if col in all_metric_cols]\n",
    "        missing = [col for col in SELECTED_METRICS if col not in all_metric_cols]\n",
    "        print(f\"\\nWill analyze {len(metric_cols)} selected metrics\")\n",
    "        if missing:\n",
    "            print(f\"Warning: {len(missing)} selected metrics not found: {missing}\")\n",
    "    \n",
    "    print(\"Selected metrics:\")\n",
    "    for i, metric in enumerate(metric_cols, 1):\n",
    "        print(f\"  {i:2d}. {metric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for R packages (skip installation attempts to avoid compilation issues)\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import rpy2.robjects as ro\n",
    "    from rpy2.robjects import pandas2ri\n",
    "    from rpy2.robjects.packages import importr, isinstalled\n",
    "\n",
    "    # Check if R is available\n",
    "    ro.r('R.version.string')\n",
    "\n",
    "    # Check if packages are already installed (don't try to install)\n",
    "    if isinstalled('lmerTest') and isinstalled('emmeans'):\n",
    "        # Try to import packages\n",
    "        lmerTest = importr('lmerTest')\n",
    "        emmeans = importr('emmeans')\n",
    "        USE_R = True\n",
    "        print(\"R packages found and loaded successfully\")\n",
    "    else:\n",
    "        print(\"R packages not installed. Using Python-based statistical analysis instead.\")\n",
    "        print(\"(To use R, install packages manually in R with: install.packages(c('lmerTest', 'emmeans')))\")\n",
    "        USE_R = False\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"R not available or packages missing. Using Python-based statistical analysis instead.\")\n",
    "    USE_R = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statistical libraries\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import f_oneway, tukey_hsd\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "if USE_R:\n",
    "    import rpy2.robjects as robjects\n",
    "    from rpy2.robjects import pandas2ri\n",
    "    from rpy2.robjects.packages import importr\n",
    "    from rpy2.robjects.conversion import localconverter\n",
    "\n",
    "def run_lmer_python(df, dv, feature_label, verbose=False):\n",
    "    \"\"\"\n",
    "    Fallback: Use Python statsmodels for mixed effects modeling\n",
    "    \"\"\"\n",
    "    # Check which columns are available\n",
    "    required_cols = [\"participant\", \"condition\", dv]\n",
    "    optional_cols = [\"session_order_numeric\", \"window_index\"]\n",
    "    \n",
    "    # Build column list based on what's available\n",
    "    cols = required_cols.copy()\n",
    "    for col in optional_cols:\n",
    "        if col in df.columns:\n",
    "            cols.append(col)\n",
    "    \n",
    "    # Check if the dependent variable exists\n",
    "    if dv not in df.columns:\n",
    "        if verbose:\n",
    "            print(f\"Column {dv} not found in dataframe\")\n",
    "        return None, None, None\n",
    "    \n",
    "    dat = df[cols].dropna().copy()\n",
    "\n",
    "    if len(dat) < 20:\n",
    "        return None, None, None\n",
    "\n",
    "    dat[\"condition\"] = pd.Categorical(dat[\"condition\"], categories=[\"L\", \"M\", \"H\"], ordered=True)\n",
    "\n",
    "    try:\n",
    "        # Build formula based on available columns\n",
    "        formula_parts = [f'{dv} ~ C(condition)']\n",
    "        random_parts = []\n",
    "        \n",
    "        if \"session_order_numeric\" in dat.columns:\n",
    "            formula_parts.append('session_order_numeric')\n",
    "        \n",
    "        if \"window_index\" in dat.columns:\n",
    "            formula_parts.append('window_index')\n",
    "            random_parts.append('window_index')\n",
    "        \n",
    "        formula = ' + '.join(formula_parts)\n",
    "        re_formula = '~' + ' + '.join(random_parts) if random_parts else None\n",
    "        \n",
    "        # Try mixed effects model if we have random effects\n",
    "        if re_formula and \"participant\" in dat.columns:\n",
    "            model = smf.mixedlm(formula, dat, groups=dat[\"participant\"],\n",
    "                               re_formula=re_formula)\n",
    "            result = model.fit(method='nm', maxiter=100)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\n=== {feature_label} (Python statsmodels) ===\")\n",
    "                print(result.summary())\n",
    "        else:\n",
    "            # Fall back to OLS if no random effects\n",
    "            model = smf.ols(formula, dat)\n",
    "            result = model.fit()\n",
    "\n",
    "        # Calculate marginal means\n",
    "        conditions = ['L', 'M', 'H']\n",
    "        means = {}\n",
    "        cis = {}\n",
    "\n",
    "        for cond in conditions:\n",
    "            cond_data = dat[dat['condition'] == cond][dv]\n",
    "            if len(cond_data) > 0:\n",
    "                means[cond] = cond_data.mean()\n",
    "                sem = cond_data.sem()\n",
    "                cis[cond] = (means[cond] - 1.96*sem, means[cond] + 1.96*sem)\n",
    "            else:\n",
    "                means[cond] = np.nan\n",
    "                cis[cond] = (np.nan, np.nan)\n",
    "\n",
    "        # Perform pairwise comparisons\n",
    "        pairwise_p = {}\n",
    "        \n",
    "        # Try Tukey HSD if we have enough data\n",
    "        try:\n",
    "            groups_data = []\n",
    "            for cond in ['L', 'M', 'H']:\n",
    "                group = dat[dat['condition'] == cond][dv].values\n",
    "                if len(group) > 0:\n",
    "                    groups_data.append(group)\n",
    "            \n",
    "            if len(groups_data) == 3:\n",
    "                tukey_result = tukey_hsd(*groups_data)\n",
    "                pairwise_p = {\n",
    "                    ('L', 'M'): tukey_result.pvalue[0, 1] if tukey_result.pvalue.shape[0] > 0 else np.nan,\n",
    "                    ('L', 'H'): tukey_result.pvalue[0, 2] if tukey_result.pvalue.shape[0] > 0 else np.nan,\n",
    "                    ('M', 'H'): tukey_result.pvalue[1, 2] if tukey_result.pvalue.shape[0] > 1 else np.nan\n",
    "                }\n",
    "            else:\n",
    "                # Fall back to pairwise t-tests\n",
    "                for (c1, c2) in [('L', 'M'), ('L', 'H'), ('M', 'H')]:\n",
    "                    g1 = dat[dat['condition'] == c1][dv].values\n",
    "                    g2 = dat[dat['condition'] == c2][dv].values\n",
    "                    if len(g1) > 0 and len(g2) > 0:\n",
    "                        _, p = stats.ttest_ind(g1, g2)\n",
    "                        pairwise_p[(c1, c2)] = p\n",
    "                    else:\n",
    "                        pairwise_p[(c1, c2)] = np.nan\n",
    "                        \n",
    "        except Exception as e:\n",
    "            # Simple pairwise t-tests as last resort\n",
    "            for (c1, c2) in [('L', 'M'), ('L', 'H'), ('M', 'H')]:\n",
    "                g1 = dat[dat['condition'] == c1][dv].values\n",
    "                g2 = dat[dat['condition'] == c2][dv].values\n",
    "                if len(g1) > 0 and len(g2) > 0:\n",
    "                    _, p = stats.ttest_ind(g1, g2)\n",
    "                    pairwise_p[(c1, c2)] = p\n",
    "                else:\n",
    "                    pairwise_p[(c1, c2)] = np.nan\n",
    "\n",
    "        return pairwise_p, means, cis\n",
    "\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Error fitting model for {feature_label}: {e}\")\n",
    "        # Fallback to simple ANOVA\n",
    "        try:\n",
    "            groups = []\n",
    "            for c in ['L', 'M', 'H']:\n",
    "                group = dat[dat['condition'] == c][dv].values\n",
    "                if len(group) > 0:\n",
    "                    groups.append(group)\n",
    "            \n",
    "            if len(groups) >= 2:\n",
    "                f_stat, p_val = f_oneway(*groups) if len(groups) > 2 else stats.ttest_ind(*groups)\n",
    "\n",
    "                # Calculate means and CIs\n",
    "                means = {}\n",
    "                cis = {}\n",
    "                for cond in ['L', 'M', 'H']:\n",
    "                    cond_data = dat[dat['condition'] == cond][dv]\n",
    "                    if len(cond_data) > 0:\n",
    "                        means[cond] = cond_data.mean()\n",
    "                        sem = cond_data.sem()\n",
    "                        cis[cond] = (means[cond] - 1.96*sem, means[cond] + 1.96*sem)\n",
    "                    else:\n",
    "                        means[cond] = np.nan\n",
    "                        cis[cond] = (np.nan, np.nan)\n",
    "\n",
    "                # Simple pairwise t-tests\n",
    "                pairwise_p = {}\n",
    "                for (c1, c2) in [('L', 'M'), ('L', 'H'), ('M', 'H')]:\n",
    "                    g1 = dat[dat['condition'] == c1][dv].values\n",
    "                    g2 = dat[dat['condition'] == c2][dv].values\n",
    "                    if len(g1) > 0 and len(g2) > 0:\n",
    "                        _, p = stats.ttest_ind(g1, g2)\n",
    "                        pairwise_p[(c1, c2)] = p\n",
    "                    else:\n",
    "                        pairwise_p[(c1, c2)] = np.nan\n",
    "\n",
    "                return pairwise_p, means, cis\n",
    "            else:\n",
    "                return None, None, None\n",
    "\n",
    "        except Exception as e2:\n",
    "            if verbose:\n",
    "                print(f\"All fallbacks failed: {e2}\")\n",
    "            return None, None, None\n",
    "\n",
    "def run_lmer_rpy2(df, dv, feature_label, verbose=False):\n",
    "    \"\"\"\n",
    "    Fit a mixed effects model with lmerTest::lmer and do pairwise emmeans.\n",
    "    Returns:\n",
    "        pairwise_p: dict {('L','M'), ('L','H'), ('M','H') -> p-value}\n",
    "        means:      dict {'L','M','H' -> emmean}\n",
    "        cis:        dict {'L','M','H' -> (lower, upper)}\n",
    "    \"\"\"\n",
    "    # Check which columns are available\n",
    "    required_cols = [\"participant\", \"condition\", dv]\n",
    "    optional_cols = [\"session_order_numeric\", \"window_index\"]\n",
    "    \n",
    "    # Build column list based on what's available\n",
    "    cols = required_cols.copy()\n",
    "    for col in optional_cols:\n",
    "        if col in df.columns:\n",
    "            cols.append(col)\n",
    "    \n",
    "    # Check if the dependent variable exists\n",
    "    if dv not in df.columns:\n",
    "        if verbose:\n",
    "            print(f\"Column {dv} not found in dataframe\")\n",
    "        return None, None, None\n",
    "    \n",
    "    dat = df[cols].dropna().copy()\n",
    "    \n",
    "    if len(dat) < 20:\n",
    "        return None, None, None\n",
    "    \n",
    "    dat = dat.rename(columns={dv: \"dv\"})\n",
    "    dat[\"condition\"] = pd.Categorical(dat[\"condition\"], categories=[\"L\", \"M\", \"H\"], ordered=True)\n",
    "\n",
    "    # Import R packages\n",
    "    lmerTest = importr(\"lmerTest\")\n",
    "    emmeans = importr(\"emmeans\")\n",
    "    base = importr(\"base\")\n",
    "\n",
    "    # Convert to R\n",
    "    with localconverter(robjects.default_converter + pandas2ri.converter):\n",
    "        r_dat = robjects.conversion.py2rpy(dat)\n",
    "\n",
    "    robjects.globalenv[\"dat\"] = r_dat\n",
    "    \n",
    "    # Build R formula based on available columns\n",
    "    r_setup = \"\"\"\n",
    "        dat$participant <- factor(dat$participant)\n",
    "        dat$condition <- factor(dat$condition, levels = c(\"L\",\"M\",\"H\"), ordered = TRUE)\n",
    "    \"\"\"\n",
    "    \n",
    "    formula_parts = [\"dv ~ condition\"]\n",
    "    \n",
    "    if \"session_order_numeric\" in dat.columns:\n",
    "        r_setup += \"\\n        dat$session_order_numeric <- as.numeric(dat$session_order_numeric)\"\n",
    "        formula_parts.append(\"session_order_numeric\")\n",
    "    \n",
    "    if \"window_index\" in dat.columns:\n",
    "        r_setup += \"\\n        dat$window_index <- as.numeric(dat$window_index)\"\n",
    "        formula_parts.append(\"window_index\")\n",
    "        random_part = \"(window_index|participant)\"\n",
    "    else:\n",
    "        random_part = \"(1|participant)\"\n",
    "    \n",
    "    robjects.r(r_setup)\n",
    "\n",
    "    # Fit model\n",
    "    formula = ' + '.join(formula_parts) + ' + ' + random_part\n",
    "    \n",
    "    try:\n",
    "        robjects.r(f\"fit <- lmerTest::lmer({formula}, data = dat)\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n=== {feature_label} (R lmerTest) ===\")\n",
    "            print(robjects.r(\"summary(fit)\"))\n",
    "\n",
    "        # Get emmeans and pairwise\n",
    "        robjects.r(\"emm <- emmeans::emmeans(fit, ~ condition)\")\n",
    "\n",
    "        emm_df = robjects.r(\"as.data.frame(emm)\")\n",
    "        ci_df = robjects.r(\"as.data.frame(confint(emm, level = 0.95))\")\n",
    "        pwc_df = robjects.r('as.data.frame(pairs(emm, adjust = \"tukey\"))')\n",
    "\n",
    "        with localconverter(robjects.default_converter + pandas2ri.converter):\n",
    "            emm_pd = robjects.conversion.rpy2py(emm_df)\n",
    "            ci_pd = robjects.conversion.rpy2py(ci_df)\n",
    "            pwc_pd = robjects.conversion.rpy2py(pwc_df)\n",
    "\n",
    "        # Extract results\n",
    "        means = {str(r[\"condition\"]): float(r[\"emmean\"]) for _, r in emm_pd.iterrows()}\n",
    "\n",
    "        lower_col = next((c for c in ci_pd.columns if \"lower\" in c.lower()), None)\n",
    "        upper_col = next((c for c in ci_pd.columns if \"upper\" in c.lower()), None)\n",
    "        \n",
    "        if lower_col and upper_col:\n",
    "            cis = {str(r[\"condition\"]): (float(r[lower_col]), float(r[upper_col])) for _, r in ci_pd.iterrows()}\n",
    "        else:\n",
    "            # Fallback to using SE\n",
    "            se_col = next((c for c in emm_pd.columns if \"se\" in c.lower() or \"std\" in c.lower()), None)\n",
    "            if se_col:\n",
    "                cis = {str(r[\"condition\"]): (float(r[\"emmean\"]) - 1.96*float(r[se_col]), \n",
    "                                             float(r[\"emmean\"]) + 1.96*float(r[se_col])) \n",
    "                      for _, r in emm_pd.iterrows()}\n",
    "            else:\n",
    "                cis = {\"L\": (np.nan, np.nan), \"M\": (np.nan, np.nan), \"H\": (np.nan, np.nan)}\n",
    "\n",
    "        # Pairwise p-values\n",
    "        pcol = \"p.value\" if \"p.value\" in pwc_pd.columns else next((c for c in pwc_pd.columns if c.lower().startswith(\"p\")), None)\n",
    "        pairwise_p = {}\n",
    "        if pcol:\n",
    "            for _, r in pwc_pd.iterrows():\n",
    "                contrast = str(r[\"contrast\"]).replace(\" \", \"\")\n",
    "                g1, g2 = contrast.split(\"-\")\n",
    "                pairwise_p[(g1, g2)] = float(r[pcol])\n",
    "\n",
    "        return pairwise_p, means, cis\n",
    "        \n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Error fitting model for {feature_label}: {e}\")\n",
    "        # Fall back to Python analysis\n",
    "        return run_lmer_python(df, dv, feature_label, verbose)\n",
    "\n",
    "print(\"Statistical functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run statistical analysis on all metrics for all normalization methods\n",
    "results = {}\n",
    "\n",
    "for method, df in pose_data.items():\n",
    "    print(f\"\\nAnalyzing {method} normalization method...\")\n",
    "    results[method] = {}\n",
    "    \n",
    "    # Group metrics by feature type for organized analysis\n",
    "    metric_groups = {\n",
    "        \"Head Rotation\": [m for m in metric_cols if \"head_rotation\" in m],\n",
    "        \"Blink\": [m for m in metric_cols if \"blink\" in m],\n",
    "        \"Mouth\": [m for m in metric_cols if \"mouth\" in m],\n",
    "        \"Pupil\": [m for m in metric_cols if \"pupil\" in m],\n",
    "        \"Center Face\": [m for m in metric_cols if \"center_face\" in m],\n",
    "    }\n",
    "    \n",
    "    # Add any Procrustes-specific metrics if present\n",
    "    if method.startswith(\"procrustes\"):\n",
    "        procrustes_metrics = [m for m in metric_cols if \"head_tx\" in m or \"head_ty\" in m or \n",
    "                             \"head_scale\" in m or \"head_motion\" in m]\n",
    "        if procrustes_metrics:\n",
    "            metric_groups[\"Procrustes Transform\"] = procrustes_metrics\n",
    "    \n",
    "    for group_name, metrics in metric_groups.items():\n",
    "        if not metrics:\n",
    "            continue\n",
    "            \n",
    "        print(f\"  Processing {group_name} ({len(metrics)} metrics)...\")\n",
    "        \n",
    "        for metric in metrics:\n",
    "            # Create a nice label for the metric\n",
    "            label = metric.replace(\"_\", \" \").title()\n",
    "            \n",
    "            # Run analysis using R if available, otherwise Python\n",
    "            if USE_R:\n",
    "                pvals, means, cis = run_lmer_rpy2(df, metric, f\"{method}_{metric}\")\n",
    "            else:\n",
    "                pvals, means, cis = run_lmer_python(df, metric, f\"{method}_{metric}\")\n",
    "            \n",
    "            if pvals is not None:\n",
    "                results[method][metric] = {\n",
    "                    \"pvals\": pvals,\n",
    "                    \"means\": means,\n",
    "                    \"cis\": cis,\n",
    "                    \"label\": label,\n",
    "                    \"group\": group_name\n",
    "                }\n",
    "\n",
    "print(f\"\\n\\nAnalysis complete! Processed {sum(len(r) for r in results.values())} metrics total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define plotting function with same styling as original\n",
    "col_pal = ['#c6dbef',  # Low - light blue\n",
    "           '#6baed6',  # Moderate - medium blue\n",
    "           '#2171b5']  # High - dark blue\n",
    "\n",
    "def barplot_ax(ax, means, sems, pvals,\n",
    "               ylabel, metric_name,\n",
    "               colors=col_pal,\n",
    "               bar_width=0.80,\n",
    "               ylim_padding=(0.4, 0.1)):\n",
    "    \"\"\"\n",
    "    Draws a bar plot with error bars and significance brackets.\n",
    "    \"\"\"\n",
    "    import textwrap\n",
    "\n",
    "    x = np.arange(len(means))\n",
    "    ax.bar(x, means, yerr=sems, capsize=4,\n",
    "           color=colors, width=bar_width,\n",
    "           edgecolor=\"black\")\n",
    "\n",
    "    # Compute lower and upper whiskers\n",
    "    lowers = [m - (s if not np.isnan(s) else 0) for m, s in zip(means, sems)]\n",
    "    uppers = [m + (s if not np.isnan(s) else 0) for m, s in zip(means, sems)]\n",
    "    y_min = min(lowers)\n",
    "    y_max = max(uppers)\n",
    "    y_span = y_max - y_min if y_max > y_min else 1.0\n",
    "\n",
    "    # Significance brackets (sorted by span length)\n",
    "    pairs = [(0,1,pvals[0]), (0,2,pvals[1]), (1,2,pvals[2])]\n",
    "    sig_pairs = [(i, j, p) for (i, j, p) in pairs if p < 0.05]\n",
    "    sig_pairs = sorted(sig_pairs, key=lambda t: (t[1]-t[0]))\n",
    "\n",
    "    h_step = 0.2 * y_span\n",
    "    line_h = 0.03 * y_span\n",
    "    y0 = y_max + 0.04 * y_span\n",
    "\n",
    "    for idx, (i, j, p) in enumerate(sig_pairs):\n",
    "        y = y0 + idx * h_step\n",
    "        ax.plot([x[i], x[i], x[j], x[j]],\n",
    "                [y, y+line_h, y+line_h, y],\n",
    "                lw=1.5, color='black', clip_on=False)\n",
    "        stars = '***' if p < .001 else '**' if p < .01 else '*'\n",
    "        ax.text((x[i]+x[j])/2, y+0.25*line_h, stars,\n",
    "                ha='center', va='bottom',\n",
    "                fontsize=13, fontweight='bold',\n",
    "                color='black', clip_on=False)\n",
    "\n",
    "    # Axis formatting\n",
    "    ax.set_xlim(-0.5, len(means)-0.5)\n",
    "    ax.set_xticks([])\n",
    "\n",
    "    # Wrap long y-labels to fit (max 25 chars per line)\n",
    "    wrapped_ylabel = \"\\n\".join(textwrap.wrap(ylabel, width=25))\n",
    "    ax.set_ylabel(wrapped_ylabel, weight='bold', fontsize=12)\n",
    "\n",
    "    ax.set_ylim(y_min - ylim_padding[0]*y_span,\n",
    "                y_max + ylim_padding[1]*y_span + len(sig_pairs)*h_step)\n",
    "\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=5))\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "    ax.spines[['top','right']].set_visible(False)\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_linewidth(1.4)\n",
    "    ax.tick_params(axis='y', width=1.3, labelsize=11)\n",
    "    for lab in ax.get_yticklabels():\n",
    "        lab.set_fontweight('bold')\n",
    "\n",
    "print(\"Plotting function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PLOTTING CONFIGURATION =====\n",
    "# Configure which plots to generate\n",
    "\n",
    "# Plotting options\n",
    "SAVE_FIGURES = True\n",
    "FIGSIZE = (8, 3)  # Wider to accommodate more subplots\n",
    "MAX_PLOTS_PER_FIGURE = 4  # Maximum number of metrics per figure\n",
    "SHOW_PLOTS = True  # Set to False to only save figures without displaying\n",
    "PLOT_ALL_METRICS = True  # Set to False to only plot significant metrics\n",
    "\n",
    "# Create output directory if saving\n",
    "if SAVE_FIGURES:\n",
    "    os.makedirs(\"./figs\", exist_ok=True)\n",
    "\n",
    "# Generate plots for each normalization method\n",
    "for method, method_results in results.items():\n",
    "    if not method_results:\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n\\nPlotting {method} results...\")\n",
    "    \n",
    "    # Get all metrics and significant metrics\n",
    "    significant_metrics = []\n",
    "    all_metrics = []\n",
    "    \n",
    "    for metric, res in method_results.items():\n",
    "        all_metrics.append((metric, res))\n",
    "        # Check if any comparison is significant\n",
    "        has_sig = any(p < 0.05 for p in res[\"pvals\"].values() if not np.isnan(p))\n",
    "        if has_sig:\n",
    "            significant_metrics.append((metric, res))\n",
    "    \n",
    "    print(f\"  Total metrics analyzed: {len(all_metrics)}\")\n",
    "    print(f\"  Metrics with significant effects: {len(significant_metrics)}\")\n",
    "    \n",
    "    # Choose which metrics to plot based on configuration\n",
    "    if PLOT_ALL_METRICS:\n",
    "        metrics_to_plot = all_metrics\n",
    "        print(f\"  Will plot ALL {len(metrics_to_plot)} metrics\")\n",
    "    else:\n",
    "        metrics_to_plot = significant_metrics if significant_metrics else all_metrics\n",
    "        print(f\"  Will plot {len(metrics_to_plot)} metrics (significant only)\")\n",
    "    \n",
    "    if not metrics_to_plot:\n",
    "        print(f\"  No metrics to plot for {method}\")\n",
    "        continue\n",
    "    \n",
    "    # Group metrics by feature type for organized plotting\n",
    "    metric_groups = {\n",
    "        \"Head Rotation\": [],\n",
    "        \"Blink\": [],\n",
    "        \"Mouth\": [],\n",
    "        \"Pupil\": [],\n",
    "        \"Center Face\": [],\n",
    "        \"Procrustes Transform\": []\n",
    "    }\n",
    "    \n",
    "    # Categorize metrics\n",
    "    for metric, res in metrics_to_plot:\n",
    "        if \"head_rotation\" in metric:\n",
    "            metric_groups[\"Head Rotation\"].append((metric, res))\n",
    "        elif \"blink\" in metric:\n",
    "            metric_groups[\"Blink\"].append((metric, res))\n",
    "        elif \"mouth\" in metric:\n",
    "            metric_groups[\"Mouth\"].append((metric, res))\n",
    "        elif \"pupil\" in metric:\n",
    "            metric_groups[\"Pupil\"].append((metric, res))\n",
    "        elif \"center_face\" in metric:\n",
    "            metric_groups[\"Center Face\"].append((metric, res))\n",
    "        elif any(x in metric for x in [\"head_tx\", \"head_ty\", \"head_scale\", \"head_motion\"]):\n",
    "            metric_groups[\"Procrustes Transform\"].append((metric, res))\n",
    "        else:\n",
    "            # Default to Center Face for unrecognized metrics\n",
    "            metric_groups[\"Center Face\"].append((metric, res))\n",
    "    \n",
    "    # Create plots for each group that has metrics\n",
    "    for group_name, group_metrics in metric_groups.items():\n",
    "        if not group_metrics:\n",
    "            continue\n",
    "            \n",
    "        print(f\"  Plotting {group_name}: {len(group_metrics)} metrics\")\n",
    "        \n",
    "        # Create multiple figures if needed (chunk metrics)\n",
    "        for chunk_idx in range(0, len(group_metrics), MAX_PLOTS_PER_FIGURE):\n",
    "            chunk = group_metrics[chunk_idx:chunk_idx + MAX_PLOTS_PER_FIGURE]\n",
    "            n_plots = len(chunk)\n",
    "            \n",
    "            # Create figure\n",
    "            if n_plots == 1:\n",
    "                fig, ax = plt.subplots(1, 1, figsize=(FIGSIZE[0]//2, FIGSIZE[1]))\n",
    "                axes = [ax]\n",
    "            else:\n",
    "                cols = min(n_plots, 4)  # Max 4 columns\n",
    "                rows = (n_plots + cols - 1) // cols  # Calculate needed rows\n",
    "                fig, axes = plt.subplots(rows, cols, figsize=(FIGSIZE[0], FIGSIZE[1] * rows))\n",
    "                if rows == 1:\n",
    "                    axes = axes if n_plots > 1 else [axes]\n",
    "                else:\n",
    "                    axes = axes.flatten()\n",
    "            \n",
    "            # Plot each metric in the chunk\n",
    "            for idx, (metric, res) in enumerate(chunk):\n",
    "                if idx >= len(axes):\n",
    "                    break\n",
    "                    \n",
    "                ax = axes[idx]\n",
    "                order = ['L', 'M', 'H']\n",
    "                means = [res[\"means\"].get(lvl, np.nan) for lvl in order]\n",
    "                cis = [res[\"cis\"].get(lvl, (np.nan, np.nan)) for lvl in order]\n",
    "                \n",
    "                # Convert CIs to SEMs\n",
    "                sems = []\n",
    "                for ci in cis:\n",
    "                    if ci is not None and isinstance(ci, (tuple, list)) and len(ci) == 2:\n",
    "                        if not np.isnan(ci[0]) and not np.isnan(ci[1]):\n",
    "                            sems.append((ci[1] - ci[0]) / 3.92)\n",
    "                        else:\n",
    "                            sems.append(np.nan)\n",
    "                    else:\n",
    "                        sems.append(np.nan)\n",
    "                \n",
    "                if all(np.isnan(m) for m in means):\n",
    "                    ax.text(0.5, 0.5, f\"No data\\n{res['label']}\", \n",
    "                           ha='center', va='center', transform=ax.transAxes)\n",
    "                    ax.set_xticks([])\n",
    "                    ax.set_yticks([])\n",
    "                    continue\n",
    "                \n",
    "                # Get p-values\n",
    "                pvals = (\n",
    "                    res[\"pvals\"].get(('L','M'), np.nan),\n",
    "                    res[\"pvals\"].get(('L','H'), np.nan),\n",
    "                    res[\"pvals\"].get(('M','H'), np.nan)\n",
    "                )\n",
    "                \n",
    "                # Create bar plot\n",
    "                barplot_ax(ax, means, sems, pvals, res[\"label\"], metric_name=metric, \n",
    "                          colors=col_pal, bar_width=0.6)\n",
    "            \n",
    "            # Hide extra subplots\n",
    "            for idx in range(len(chunk), len(axes)):\n",
    "                axes[idx].set_visible(False)\n",
    "            \n",
    "            # Add overall title\n",
    "            chunk_suffix = f\"_part{chunk_idx//MAX_PLOTS_PER_FIGURE + 1}\" if len(group_metrics) > MAX_PLOTS_PER_FIGURE else \"\"\n",
    "            title = f\"{method.replace('_', ' ').title()} - {group_name}{chunk_suffix}\"\n",
    "            fig.suptitle(title, fontweight='bold', fontsize=16)\n",
    "            \n",
    "            # Adjust layout\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            if SHOW_PLOTS:\n",
    "                plt.show()\n",
    "            \n",
    "            # Save figure\n",
    "            if SAVE_FIGURES:\n",
    "                safe_group_name = group_name.replace(' ', '_').replace('/', '_').lower()\n",
    "                filename = f\"pose_{method}_{safe_group_name}{chunk_suffix}.svg\"\n",
    "                fig.savefig(f\"./figs/{filename}\", dpi=300, bbox_inches='tight')\n",
    "                print(f\"    Saved: {filename}\")\n",
    "            \n",
    "            plt.close(fig)  # Close figure to free memory\n",
    "\n",
    "print(\"\\n\\nAll plots generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary of significant effects\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY OF SIGNIFICANT EFFECTS (p < 0.05)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for method, method_results in results.items():\n",
    "    sig_effects = []\n",
    "    \n",
    "    for metric, res in method_results.items():\n",
    "        sig_comparisons = []\n",
    "        for (g1, g2), p in res[\"pvals\"].items():\n",
    "            if p < 0.05:\n",
    "                stars = \"***\" if p < 0.001 else \"**\" if p < 0.01 else \"*\"\n",
    "                sig_comparisons.append(f\"{g1}v{g2}{stars}\")\n",
    "        \n",
    "        if sig_comparisons:\n",
    "            sig_effects.append(f\"  {res['label']}: {', '.join(sig_comparisons)}\")\n",
    "    \n",
    "    if sig_effects:\n",
    "        print(f\"\\n{method.upper()} NORMALIZATION:\")\n",
    "        for effect in sig_effects:\n",
    "            print(effect)\n",
    "    else:\n",
    "        print(f\"\\n{method.upper()} NORMALIZATION: No significant effects\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"* p < 0.05, ** p < 0.01, *** p < 0.001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to CSV for further analysis\n",
    "summary_data = []\n",
    "\n",
    "for method, method_results in results.items():\n",
    "    for metric, res in method_results.items():\n",
    "        row = {\n",
    "            'normalization': method,\n",
    "            'metric': metric,\n",
    "            'group': res['group'],\n",
    "            'mean_L': res['means'].get('L', np.nan),\n",
    "            'mean_M': res['means'].get('M', np.nan),\n",
    "            'mean_H': res['means'].get('H', np.nan),\n",
    "            'p_L_M': res['pvals'].get(('L', 'M'), np.nan),\n",
    "            'p_L_H': res['pvals'].get(('L', 'H'), np.nan),\n",
    "            'p_M_H': res['pvals'].get(('M', 'H'), np.nan),\n",
    "        }\n",
    "        summary_data.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df.to_csv('./pose_stats_summary.csv', index=False)\n",
    "print(f\"Results summary saved to pose_stats_summary.csv\")\n",
    "print(f\"Total metrics analyzed: {len(summary_df)}\")\n",
    "\n",
    "# Show metrics with strongest effects (lowest p-values)\n",
    "print(\"\\nTop 10 metrics with strongest condition effects (L vs H):\")\n",
    "top_effects = summary_df.nsmallest(10, 'p_L_H')[['normalization', 'metric', 'p_L_H']]\n",
    "for _, row in top_effects.iterrows():\n",
    "    print(f\"  {row['normalization']}: {row['metric']} (p={row['p_L_H']:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matb-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
